from flask import Flask, request, jsonify
from autogen import AssistantAgent, UserProxyAgent
from autogen.agentchat.contrib.rag_agent import RAGAgent
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from chromadb.utils import embedding_functions

# ----------------------------
# Config
# ----------------------------
CHROMA_DB_PATH = "./tmp/chromadb"
CHROMA_COLLECTION = "autogen-rag-chroma"

embedding_model = "text-embedding-ada-002"   # Replace with your Azure embedding deployment
APIKEY = "your-azure-openai-key"
APIKEYENDPOINT = "https://your-resource.openai.azure.com"

# ----------------------------
# Embedding function
# ----------------------------
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=APIKEY,
    api_base=APIKEYENDPOINT,
    api_type="azure",
    model_name=embedding_model,
)

# ----------------------------
# Create vectordb (Chroma wrapper)
# ----------------------------
vectordb = Chroma(
    collection_name=CHROMA_COLLECTION,
    embedding_function=openai_ef,
    persist_directory=CHROMA_DB_PATH,
)

# ----------------------------
# Add documents (with chunking)
# ----------------------------
def add_document(doc_id: str, doc_text: str):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_text(doc_text)

    docs = [Document(page_content=chunk, metadata={"source": doc_id}) for chunk in chunks]
    vectordb.add_documents(docs)

# ----------------------------
# Setup Autogen Agents
# ----------------------------
assistant = AssistantAgent(
    name="assistant",
    llm_config={
        "model": "gpt-4o",   # Replace with your Azure deployment
        "api_key": APIKEY,
        "api_base": APIKEYENDPOINT,
        "api_type": "azure",
    },
)

rag_agent = RAGAgent(
    name="rag",
    retriever=vectordb.as_retriever(search_kwargs={"k": 3}),  # only top-3 chunks
    assistant_agent=assistant,
)

user = UserProxyAgent(name="user")

# ----------------------------
# Flask App
# ----------------------------
app = Flask(__name__)

@app.route("/upload", methods=["POST"])
def upload():
    """Upload a document and add it into ChromaDB."""
    if "file" not in request.files:
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files["file"]
    text = file.read().decode("utf-8")
    add_document(file.filename, text)

    return jsonify({"status": "uploaded", "filename": file.filename})

@app.route("/query", methods=["POST"])
def query():
    """Query RAGAgent with user input."""
    data = request.get_json()
    query_text = data.get("query", "")

    if not query_text:
        return jsonify({"error": "Query is required"}), 400

    answer = user.initiate_chat(rag_agent, message=query_text)
    return jsonify({"query": query_text, "answer": answer})

# ----------------------------
# Run the app
# ----------------------------
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)



1) High-level sequence (end-to-end)

User registers cluster in UI:

Enters cluster-id, kubeconfig or installs an agent DaemonSet using a token, and provides a Git repo URL (manifest repo / helm values repo) + service account with PR creation rights (or a deploy pipeline token).

In-cluster KubeMind Agent starts sending telemetry (Prometheus metrics, node/pod metadata) to Control Plane.

Control Plane stores data (TimescaleDB / Postgres) and triggers initial inventory scan: clusters, nodes, pods, requests/limits, deployments, helm charts, Kustomize overlays.

User clicks Run AI Analysis in UI (or schedules on cron).

Orchestrator (Coordinator Agent) starts analysis job: it calls ML Engine for forecasts and anomaly detection, then invokes multiple MCP agents in parallel:

Optimizer Agent (pod/node rightsizing & bin packing)

FinOps Agent (cost breakdown & budget suggestions)

Security Agent (misconfigs, RBAC, secrets)

Repo Agent (understand manifests & generate PR patch)

Each agent calls MCP tools to get cluster state, metrics, and cloud pricing and returns structured recommendations (JSON) to the Coordinator.

Coordinator validates recommendations against policy guardrails and SLOs; produces final suggestions shown in UI.

If user approves in UI, the Repo Agent:

Maps recommended changes to repository files (helm values, kustomize overlays, raw manifests)

Generates patch/diff

Creates a Git PR with description, rationale, estimated savings and test plan.

CI/CD pipeline runs PR checks (unit tests, lint, perhaps small canary deploy in staging).

After approval/merge, GitOps (ArgoCD/Flux) deploys changes to cluster or RightsizeRequest CRD triggers operator to do canary apply.

Monitor for regressions; auto-rollback policy if SLOs are violated after change.
