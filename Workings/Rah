from flask import Flask, request, jsonify
from autogen import AssistantAgent, UserProxyAgent
from autogen.agentchat.contrib.rag_agent import RAGAgent
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from chromadb.utils import embedding_functions

# ----------------------------
# Config
# ----------------------------
CHROMA_DB_PATH = "./tmp/chromadb"
CHROMA_COLLECTION = "autogen-rag-chroma"

embedding_model = "text-embedding-ada-002"   # Replace with your Azure embedding deployment
APIKEY = "your-azure-openai-key"
APIKEYENDPOINT = "https://your-resource.openai.azure.com"

# ----------------------------
# Embedding function
# ----------------------------
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=APIKEY,
    api_base=APIKEYENDPOINT,
    api_type="azure",
    model_name=embedding_model,
)

# ----------------------------
# Create vectordb (Chroma wrapper)
# ----------------------------
vectordb = Chroma(
    collection_name=CHROMA_COLLECTION,
    embedding_function=openai_ef,
    persist_directory=CHROMA_DB_PATH,
)

# ----------------------------
# Add documents (with chunking)
# ----------------------------
def add_document(doc_id: str, doc_text: str):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_text(doc_text)

    docs = [Document(page_content=chunk, metadata={"source": doc_id}) for chunk in chunks]
    vectordb.add_documents(docs)

# ----------------------------
# Setup Autogen Agents
# ----------------------------
assistant = AssistantAgent(
    name="assistant",
    llm_config={
        "model": "gpt-4o",   # Replace with your Azure deployment
        "api_key": APIKEY,
        "api_base": APIKEYENDPOINT,
        "api_type": "azure",
    },
)

rag_agent = RAGAgent(
    name="rag",
    retriever=vectordb.as_retriever(search_kwargs={"k": 3}),  # only top-3 chunks
    assistant_agent=assistant,
)

user = UserProxyAgent(name="user")

# ----------------------------
# Flask App
# ----------------------------
app = Flask(__name__)

@app.route("/upload", methods=["POST"])
def upload():
    """Upload a document and add it into ChromaDB."""
    if "file" not in request.files:
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files["file"]
    text = file.read().decode("utf-8")
    add_document(file.filename, text)

    return jsonify({"status": "uploaded", "filename": file.filename})

@app.route("/query", methods=["POST"])
def query():
    """Query RAGAgent with user input."""
    data = request.get_json()
    query_text = data.get("query", "")

    if not query_text:
        return jsonify({"error": "Query is required"}), 400

    answer = user.initiate_chat(rag_agent, message=query_text)
    return jsonify({"query": query_text, "answer": answer})

# ----------------------------
# Run the app
# ----------------------------
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)



1) High-level sequence (end-to-end)

User registers cluster in UI:

Enters cluster-id, kubeconfig or installs an agent DaemonSet using a token, and provides a Git repo URL (manifest repo / helm values repo) + service account with PR creation rights (or a deploy pipeline token).

In-cluster KubeMind Agent starts sending telemetry (Prometheus metrics, node/pod metadata) to Control Plane.

Control Plane stores data (TimescaleDB / Postgres) and triggers initial inventory scan: clusters, nodes, pods, requests/limits, deployments, helm charts, Kustomize overlays.

User clicks Run AI Analysis in UI (or schedules on cron).

Orchestrator (Coordinator Agent) starts analysis job: it calls ML Engine for forecasts and anomaly detection, then invokes multiple MCP agents in parallel:

Optimizer Agent (pod/node rightsizing & bin packing)

FinOps Agent (cost breakdown & budget suggestions)

Security Agent (misconfigs, RBAC, secrets)

Repo Agent (understand manifests & generate PR patch)

Each agent calls MCP tools to get cluster state, metrics, and cloud pricing and returns structured recommendations (JSON) to the Coordinator.

Coordinator validates recommendations against policy guardrails and SLOs; produces final suggestions shown in UI.

If user approves in UI, the Repo Agent:

Maps recommended changes to repository files (helm values, kustomize overlays, raw manifests)

Generates patch/diff

Creates a Git PR with description, rationale, estimated savings and test plan.

CI/CD pipeline runs PR checks (unit tests, lint, perhaps small canary deploy in staging).

After approval/merge, GitOps (ArgoCD/Flux) deploys changes to cluster or RightsizeRequest CRD triggers operator to do canary apply.

Monitor for regressions; auto-rollback policy if SLOs are violated after change.
"""
OVERVIEW â€“ Metrics Collector Purpose

Goal:
To gather the right subset of metrics (not all!) from:

AKS (Kubernetes cluster level)

Azure Monitor / Resource Graph / Cost Management

Workload layer (Pods, Deployments, Namespaces)

Node layer (VM metrics, GPU, CPU, Disk, Network)
â€¦and send them to the Feature Store for ML analysis.



---

ðŸ§© 1. Metrics Categories and Tools

Layer	Tool	Metrics Collected	Granularity	Collection Mode

Pod/Container	Prometheus + cAdvisor	CPU (usage, limit), Memory (usage, working set), Restarts, Pod OOMKill, Latency (P99)	Per pod, per 10s	Prometheus scrape from kubelet/cAdvisor
Node/VM	Prometheus Node Exporter	CPU idle, load avg, disk I/O, network I/O, file system usage	Per node	DaemonSet scrape
Cluster (AKS)	kube-state-metrics + AKS Metrics API	Pod count, Node count, ReplicaSets, Pending Pods, Resource requests/limits, Pod eviction rate	Cluster-wide	Prometheus scrape + Azure Monitor API
Azure Resource Layer	Azure Monitor Metrics API	VM CPU %, Memory %, Network throughput, Disk read/write ops, AKS control plane metrics, Network Load Balancer health	Per resource (VM, disk, LB, NSG)	REST API pull or Log Analytics query
Cost & Billing	Azure Cost Management + Resource Graph	Resource cost, namespace-level cost, node pool cost, SKU, Spot vs On-demand	Daily	REST API query
Application Layer (optional)	OpenTelemetry / App Insights	Request rate, error rate, latency, dependency failures	Per service	SDK instrumentation



---

ðŸ§  2. What to Collect for Our Use Case

Your goal = Optimization + Cost + Performance + Scaling Intelligence
So, we only collect high-value signals that help predict:

Under/over-utilization

Cost inefficiency

Autoscaling opportunities

Performance degradation


Letâ€™s shortlist per layer ðŸ‘‡


---

ðŸ”¹ Pod-level Metrics (from Prometheus / cAdvisor)

Metric	Reason

container_cpu_usage_seconds_total	CPU utilization % (core utilization trend)
container_memory_working_set_bytes	Memory usage pattern
kube_pod_container_resource_requests_cpu_cores	Requested CPU for comparison
kube_pod_container_resource_limits_cpu_cores	Limits vs actual usage
kube_pod_status_ready	Availability
kube_pod_container_status_restarts_total	Instability indicator
kube_pod_start_time	Pod lifetime for scaling prediction


ðŸ“ How: Prometheus scrapes from kubelet and cAdvisor (via /metrics endpoint on each node).


---

ðŸ”¹ Node-level Metrics (from Node Exporter)

Metric	Reason

node_cpu_seconds_total	Total node CPU usage
node_memory_MemAvailable_bytes	Memory pressure detection
node_disk_read_bytes_total / write_bytes_total	Disk I/O load
node_network_receive_bytes_total / transmit_bytes_total	Network throughput for autoscaling
node_load1, node_load5	Overall node stress level


ðŸ“ How: Installed as a DaemonSet. Exposes /metrics endpoint on each node.


---

ðŸ”¹ Cluster / AKS-level Metrics (from Azure Monitor + kube-state-metrics)

Metric	Reason

kube_deployment_status_replicas_available	Replicas vs desired
kube_pod_status_phase	Pending pods detection
kube_node_status_allocatable_cpu_cores	Cluster resource ceiling
kube_node_status_condition{condition="Ready"}	Node health check
container_cpu_allocation (AKS metric)	Actual CPU allocated in the node pool
container_memory_rss_bytes	Memory footprint tracking


ðŸ“ How:

kube-state-metrics â†’ Prometheus scrape

AKS â†’ Azure Monitor REST API /subscriptions/{id}/providers/Microsoft.ContainerService/managedClusters/{cluster}/providers/microsoft.insights/metrics



---

ðŸ”¹ Azure Resource Metrics (from Azure Monitor)

Metric	Resource	Reason

Percentage CPU	VM	Detect idle/overload node pools
Network In/Out Total	VM / LB	Identify network bottlenecks
Disk Queue Depth	Disk	Detect I/O saturation
Memory Usage	VM	Detect memory pressure
Egress	Storage / Network	Identify high traffic patterns


ðŸ“ How: Use Azure Monitor Metrics REST API or Log Analytics KQL query.
Data pulled every 5â€“15 min into ETL pipeline.


---

ðŸ”¹ Cost Metrics (from Azure Cost Management API)

Metric	Scope	Reason

PreTaxCost	Resource / Namespace / Cluster	Cost breakdown by workload
MeterCategory, MeterName	Compute, Storage, Network	Identify cost drivers
UsageQuantity	Resource level	Actual consumption
CostUSD	Aggregated	Daily spend trend


ðŸ“ How:
Azure REST API endpoint â†’ /providers/Microsoft.CostManagement/query
Filter by ResourceGroup, Namespace, ServiceName.


---

ðŸ“¦ 3. Data Pipeline (How Metrics Are Collected & Used)

[AKS / Azure Resources]
        â”‚
        â”œâ”€â–º Prometheus (Pod, Node, Cluster)
        â”œâ”€â–º Azure Monitor API (Infra)
        â”œâ”€â–º Azure Cost Mgmt API (Cost)
        â”‚
        â–¼
 [ETL Layer]
   - Cleans, merges, normalizes timestamps
   - Filters only top metrics (based on config)
        â”‚
        â–¼
 [Feature Store]
   - Used by ML Engine for:
       - Resource prediction
       - Cost anomaly detection
       - Pod scaling recommendation


---

âš™ï¸ 4. How You Select Metrics (Logic)

We donâ€™t collect everything.
We define a â€œmetrics manifestâ€ YAML file, example:

metrics_manifest:
  pod:
    - container_cpu_usage_seconds_total
    - container_memory_working_set_bytes
    - kube_pod_status_ready
  node:
    - node_cpu_seconds_total
    - node_memory_MemAvailable_bytes
  cluster:
    - kube_deployment_status_replicas_available
  azure:
    - Percentage CPU
    - Memory Usage
  cost:
    - PreTaxCost
    - UsageQuantity

The collector only pulls those defined metrics, reducing cost & noise.


---

ðŸ’¡ 5. Why These Metrics (and Not 1000s)

AKS exposes >1000 metrics, but:

80% of optimization signals come from <50 metrics.

Collecting all creates storage + compute overhead.

Your ML engine and optimization logic need only correlated performance, utilization, and cost signals.


Hence, your collector becomes configurable and intelligent, not just a blind scraper.


---

âœ… Summary â€“ What Youâ€™ll Have

Tools Used: Prometheus, Node Exporter, kube-state-metrics, Azure Monitor API, Azure Cost Management API

Data Collected: ~40â€“50 curated metrics

Flow: AKS + Azure â†’ ETL â†’ Feature Store â†’ ML â†’ Optimization Agents

Outcome:

Identify under/over-utilized pods/nodes

Predict cost & scaling impact

Enable MCP agent for validated optimization

Data Capture: Collect logs, metrics, and traces from all AKS namespaces and microservices.


2. Event Streaming: Route data via Kafka/Event Router for real-time processing.


3. Data Normalization: Standardize and enrich events for consistency across systems.


4. Storage Layer: Store processed data in Elastic, SQL, and time-series databases.


5. AI/ML Inference: Use anomaly detection and correlation models for root-cause analysis.


6. AI Agents: Domain, anomaly, and remediation agents collaborate for intelligent insights.


7. Visualization: Insights are surfaced in Grafana dashboards and AIOps console.


8. Automation: Trigger auto-remediation or workflow actions via bots or pipelines.


9. Continuous Learning: Feedback from incidents retrains models to improve accuracy.

"""
